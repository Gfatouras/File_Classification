{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceb6969a-1776-44cd-8c88-a479b48bc449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained model...\n",
      "Tokenizer loaded successfully.\n",
      "Loading the LabelEncoder...\n",
      "Loading the training configuration...\n",
      "Extracted metadata for 37_2018_Khosla_Ventures_Seed_L.P._-_Audited_Financial_Statements.pdf: Name: 37_2018_Khosla_Ventures_Seed_L.P._-_Audited_Financial_Statements.pdf, File extension: pdf, Size (MB): 0.67, Created by: AlphaBeta, Last modified by: AlphaBeta, Created date: 2024-11-25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "                                                Name File extension  \\\n",
      "0  37_2018_Khosla_Ventures_Seed_L.P._-_Audited_Fi...            pdf   \n",
      "\n",
      "   Size (MB) Created by Last modified by  Input  \\\n",
      "0       0.67  AlphaBeta        AlphaBeta    NaN   \n",
      "\n",
      "                                 Predicted Directory  \n",
      "0  FundDocumentation > Shared Documents > a. Due ...  \n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinterdnd2 import TkinterDnD, DND_FILES\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Check if the script is running in an interactive environment (e.g., Jupyter)\n",
    "if getattr(sys, 'frozen', False):  # Running as an executable (frozen app)\n",
    "    application_path = os.path.dirname(sys.executable)\n",
    "elif '__file__' in globals():  # Running as a script\n",
    "    application_path = os.path.dirname(__file__)\n",
    "else:  # Interactive environment, set to current working directory\n",
    "    application_path = os.getcwd()\n",
    "\n",
    "# Adjust the paths to your resources relative to the application path\n",
    "model_path = os.path.join(application_path, \"production_model\", \"model.h5\")\n",
    "tokenizer_file = os.path.join(application_path, \"production_model\", \"tokenizer.pkl\")\n",
    "label_encoder_file = os.path.join(application_path, \"production_model\", \"labels.pkl\")\n",
    "config_file = os.path.join(application_path, \"production_model\", \"config.json\")\n",
    "\n",
    "\n",
    "\n",
    "# Paths for loading saved components\n",
    "model_path = \"production_model/model.h5\"\n",
    "tokenizer_file = \"production_model/tokenizer.pkl\"\n",
    "label_encoder_file = \"production_model/labels.pkl\"\n",
    "config_file = \"production_model/config.json\"\n",
    "\n",
    "# Global dataframe to store file metadata\n",
    "columns = [\n",
    "    \"Name\", \"File extension\", \"Size (MB)\", \"Created by\", \n",
    "    \"Last modified by\", \"Input\", \"Predicted Directory\"\n",
    "]\n",
    "data = []\n",
    "\n",
    "# Load the trained model\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading the trained model...\")\n",
    "    model = load_model(model_path)\n",
    "else:\n",
    "    print(\"Model not found. Please train and save the model.\")\n",
    "    exit()\n",
    "\n",
    "# Define the path to the saved tokenizer\n",
    "folder_path = \"production_model\"  # Make sure this is the correct path to the folder\n",
    "\n",
    "# Define the path to the saved tokenizer\n",
    "tokenizer_file = os.path.join(folder_path, \"tokenizer.pkl\")\n",
    "\n",
    "try:\n",
    "    with open(tokenizer_file, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Tokenizer file not found at {tokenizer_file}.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load the LabelEncoder\n",
    "if os.path.exists(label_encoder_file):\n",
    "    print(\"Loading the LabelEncoder...\")\n",
    "    label_encoder = joblib.load(label_encoder_file)\n",
    "else:\n",
    "    print(\"LabelEncoder not found. Please save the LabelEncoder.\")\n",
    "    exit()\n",
    "\n",
    "# Load training configuration\n",
    "if os.path.exists(config_file):\n",
    "    print(\"Loading the training configuration...\")\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        max_sequence_length = config[\"max_sequence_length\"]\n",
    "else:\n",
    "    print(\"Configuration file not found. Please save the configuration.\")\n",
    "    exit()\n",
    "\n",
    "def extract_file_metadata(file_path, custom_size=0.67, custom_created_by=\"AlphaBeta\", custom_last_modified_by=\"AlphaBeta\"):\n",
    "    \"\"\"Extract metadata for a given file, with optional custom values for a test.\"\"\"\n",
    "    try:\n",
    "        # Extract basic file info\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_extension = os.path.splitext(file_name)[1][1:]  # Remove leading dot\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "        created_time = os.path.getctime(file_path)\n",
    "        created_date = datetime.fromtimestamp(created_time).strftime('%Y-%m-%d')  # Date only\n",
    "\n",
    "        # Use custom values if provided, otherwise extract from the file\n",
    "        created_by = custom_created_by if custom_created_by else \"\"  # Leave empty if not provided\n",
    "        last_modified_by = custom_last_modified_by if custom_last_modified_by else \"\"  # Leave empty if not provided\n",
    "        file_size = custom_size if custom_size else file_size  # Override file size if custom size is provided\n",
    "\n",
    "        # Construct the input column text\n",
    "        input_text = (\n",
    "            f\"Name: {file_name}, File extension: {file_extension}, \"\n",
    "            f\"Size (MB): {round(file_size, 2)}, Created by: {created_by}, \"\n",
    "            f\"Last modified by: {last_modified_by}, Created date: {created_date}\"\n",
    "        )\n",
    "\n",
    "        print(f\"Extracted metadata for {file_name}: {input_text}\")\n",
    "        \n",
    "        return {\n",
    "            \"Name\": file_name,\n",
    "            \"File extension\": file_extension,\n",
    "            \"Size (MB)\": round(file_size, 2),\n",
    "            \"Created by\": created_by,\n",
    "            \"Last modified by\": last_modified_by,\n",
    "            \"Input\": input_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata for {file_path}: {e}\")\n",
    "        return {\"Error\": str(e)}\n",
    "\n",
    "\n",
    "def predict_directory(metadata):\n",
    "    \"\"\"Predict the target directory path using the model.\"\"\"\n",
    "    try:\n",
    "        # Extract the 'Input' feature and preprocess it\n",
    "        input_text = metadata.get('Input', '')  # Safely get the input, defaulting to empty string if None\n",
    "        \n",
    "        if input_text is None or input_text == \"\":\n",
    "            return \"Error: Input data is missing or invalid\"\n",
    "        \n",
    "        # Tokenize and pad the input text\n",
    "        tokenized_text = tokenizer.texts_to_sequences([input_text])\n",
    "        padded_text = pad_sequences(tokenized_text, maxlen=max_sequence_length)\n",
    "        \n",
    "        # Predict the directory\n",
    "        predictions = model.predict(padded_text)\n",
    "        predicted_label = label_encoder.inverse_transform([np.argmax(predictions)])\n",
    "        return predicted_label[0]\n",
    "    except Exception as e:\n",
    "        return f\"Error in prediction: {str(e)}\"\n",
    "\n",
    "def handle_drop(event):\n",
    "    \"\"\"Handle files dropped onto the window.\"\"\"\n",
    "    global data\n",
    "    file_paths = event.data.strip().splitlines()\n",
    "    output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        file_path = file_path.strip('{}')  # Remove any enclosing braces added by TkinterDnD\n",
    "        metadata = extract_file_metadata(file_path)\n",
    "        \n",
    "        if \"Error\" in metadata:\n",
    "            output_text.insert(tk.END, f\"Error processing {file_path}: {metadata['Error']}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Keep \"Input\" for prediction purposes but remove it from the display\n",
    "        input_text = metadata[\"Input\"]  # Store \"Input\" for prediction\n",
    "        metadata_display = {key: value if value is not None else \"\" for key, value in metadata.items() if key != \"Input\"}  # Handle None values\n",
    "        \n",
    "        # Predict directory for the file using the \"Input\" field\n",
    "        predicted_directory = predict_directory(metadata)\n",
    "        metadata_display[\"Predicted Directory\"] = predicted_directory if predicted_directory else \"Unknown\"\n",
    "        \n",
    "        data.append(metadata_display)\n",
    "        \n",
    "        # Calculate the longest line in the metadata (excluding \"Input\")\n",
    "        max_line_length = max(len(f\"{key}: {value}\") for key, value in metadata_display.items())\n",
    "        separator = \"=\" * max_line_length  # Create a separator of the appropriate length\n",
    "        \n",
    "        # Display extracted metadata in the desired format (without \"Input\")\n",
    "        output_text.insert(tk.END, \"Metadata:\\n\")\n",
    "        output_text.insert(tk.END, f\"{separator}\\n\")\n",
    "        for key, value in metadata_display.items():\n",
    "            output_text.insert(tk.END, f\"{key}: {value}\\n\")\n",
    "        output_text.insert(tk.END, f\"{separator}\\n\")\n",
    "    \n",
    "    # Update the dataframe\n",
    "    update_dataframe()\n",
    "\n",
    "def update_dataframe():\n",
    "    \"\"\"Update the dataframe with the collected data and display it.\"\"\"\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    print(df)  # Output to the console for verification\n",
    "    # Optionally save to CSV\n",
    "    # df.to_csv(\"file_metadata_with_predictions.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the TkinterDnD main window\n",
    "    root = TkinterDnD.Tk()  # Use TkinterDnD for drag-and-drop\n",
    "    root.title(\"Drag and Drop Files for Metadata\")\n",
    "    root.geometry(\"1200x400\")\n",
    "\n",
    "    # Label for instructions\n",
    "    label = tk.Label(root, text=\"Drag and Drop Your Files Here\", font=(\"Helvetica\", 14))\n",
    "    label.pack(pady=10)\n",
    "\n",
    "    # Create a text box to display file metadata\n",
    "    output_text = tk.Text(root, height=15, wrap=tk.WORD)\n",
    "    output_text.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    # Bind drag-and-drop event to the text box\n",
    "    root.drop_target_register(DND_FILES)\n",
    "    root.dnd_bind(\"<<Drop>>\", handle_drop)\n",
    "\n",
    "    # Start the application\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd208581-6446-418c-aff8-a9f6c26baa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
